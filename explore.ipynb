{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and display nice table.\n",
    "data = pd.read_csv('project_data/data/train.csv', delimiter=',')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells all the additional information about the dataset.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the next 3 cells back to back before running the perceptron code.\n",
    "\n",
    "# Pre-process the train data for Perceptron.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Use this scaler to normalize the train data.\n",
    "scaler = StandardScaler()\n",
    "vt = VarianceThreshold(0.01)\n",
    "\n",
    "p_x = data.iloc[:, 1:]\n",
    "p_y = data.iloc[:, 0]\n",
    "p_x = vt.fit_transform(p_x)\n",
    "p_x = scaler.fit_transform(p_x)\n",
    "\n",
    "# Replace 0 with -1 to fit design of custom perceptron implementation.\n",
    "p_y = p_y.replace(0, -1)\n",
    "\n",
    "pd.concat([p_y, pd.DataFrame(p_x)], axis=1).to_csv('output/perceptron_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the test data for Perceptron.\n",
    "test_data = pd.read_csv('project_data/data/test.csv', delimiter=',')\n",
    "\n",
    "p_test_x = test_data.iloc[:, 1:]\n",
    "p_test_y = test_data.iloc[:, 0]\n",
    "\n",
    "p_test_x = vt.transform(p_test_x)\n",
    "p_test_x = scaler.transform(p_test_x)\n",
    "\n",
    "# Replace 0 with -1 to fit design of custom perceptron implementation.\n",
    "p_test_y = p_test_y.replace(0, -1)\n",
    "\n",
    "pd.concat([p_test_y, pd.DataFrame(p_test_x)], axis=1).to_csv('output/perceptron_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the eval data for Perceptron.\n",
    "eval_data = pd.read_csv('project_data/data/eval.anon.csv', delimiter=',')\n",
    "\n",
    "p_eval_x = eval_data.iloc[:, 1:]\n",
    "p_eval_y = eval_data.iloc[:, 0]\n",
    "\n",
    "p_eval_x = vt.transform(p_eval_x)\n",
    "p_eval_x = scaler.transform(p_eval_x)\n",
    "\n",
    "# Replace 0 with -1 to fit design of custom perceptron implementation.\n",
    "p_eval_y = p_eval_y.replace(0, -1)\n",
    "\n",
    "pd.concat([p_eval_y, pd.DataFrame(p_eval_x)], axis=1).to_csv('output/perceptron_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the perceptron code in the perceptron directory to get the predictions first.\n",
    "\n",
    "# Post-process the perceptron predictions for submission.\n",
    "eval_id_data = pd.read_csv('project_data/data/eval.id', header=None)\n",
    "perceptron_margin_data = pd.read_csv('perceptron/perceptron_preds_no_id.csv', delimiter=',')\n",
    "\n",
    "# Match the eval example ids with the perceptron predictions.\n",
    "eval_id_data.columns = ['id']\n",
    "perceptron_margin_data = pd.concat([eval_id_data, perceptron_margin_data], axis=1)\n",
    "\n",
    "# Get rid of the label column since we just want the example ids and the predictions.\n",
    "perceptron_margin_data.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Rename the columns for submission.\n",
    "perceptron_margin_data.columns = ['example_id', 'label']\n",
    "\n",
    "# Save the perceptron submission data out.\n",
    "perceptron_margin_data.to_csv('output/perceptron_margin_submission.csv', index=False)\n",
    "\n",
    "perceptron_margin_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the next 3 cells back to back before running the ID3 code.\n",
    "\n",
    "# Pre-process the train data for ID3.\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Discretize the data.\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "# Remove all features with constant zeros, to speed up the algo.\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "id3_x = data.iloc[:, 1:]\n",
    "id3_y = data.iloc[:, 0]\n",
    "id3_x = vt.fit_transform(id3_x)\n",
    "id3_x_discretized = discretizer.fit_transform(id3_x)\n",
    "\n",
    "# Save the discretized data out.\n",
    "pd.concat([id3_y, pd.DataFrame(id3_x_discretized)], axis=1).to_csv('output/id3_train_discretized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the test data for ID3.\n",
    "test_data = pd.read_csv('project_data/data/test.csv', delimiter=',')\n",
    "\n",
    "id3_x = test_data.iloc[:, 1:]\n",
    "id3_y = test_data.iloc[:, 0]\n",
    "id3_x = vt.transform(id3_x)\n",
    "id3_test_x_discretized = discretizer.transform(id3_x)\n",
    "\n",
    "# Save the discretized data out.\n",
    "pd.concat([id3_y, pd.DataFrame(id3_test_x_discretized)], axis=1).to_csv('output/id3_test_discretized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the eval data for ID3.\n",
    "eval_data = pd.read_csv('project_data/data/eval.anon.csv', delimiter=',')\n",
    "\n",
    "id3_x = eval_data.iloc[:, 1:]\n",
    "id3_y = eval_data.iloc[:, 0]\n",
    "id3_x = vt.transform(id3_x)\n",
    "id3_eval_x_discretized = discretizer.transform(id3_x)\n",
    "\n",
    "# Save the discretized data out.\n",
    "pd.concat([id3_y, pd.DataFrame(id3_eval_x_discretized)], axis=1).to_csv('output/id3_eval_discretized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the id3 code in the id3 directory to get the predictions first.\n",
    "\n",
    "# Post-process the ID3 predictions for submission.\n",
    "\n",
    "eval_id_data = pd.read_csv('project_data/data/eval.id', header=None)\n",
    "id3_data = pd.read_csv('id3/id3_preds_no_id.csv', delimiter=',')\n",
    "\n",
    "# Match the eval example ids with the ID3 predictions.\n",
    "eval_id_data.columns = ['id']\n",
    "id3_data = pd.concat([eval_id_data, id3_data], axis=1)\n",
    "\n",
    "# Get rid of the label column since we just want the example ids and the predictions.\n",
    "id3_data.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Rename the columns for submission.\n",
    "id3_data.columns = ['example_id', 'label']\n",
    "\n",
    "# Save the ID3 submission data out.\n",
    "id3_data.to_csv('output/id3_submission.csv', index=False)\n",
    "\n",
    "id3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the adaboost code in the adaboost directory to get the predictions first.\n",
    "\n",
    "# Post-process the AdaBoost predictions for submission.\n",
    "eval_id_data = pd.read_csv('project_data/data/eval.id', header=None)\n",
    "adaboost_data = pd.read_csv('adaboost/adaboost_preds_no_id.csv', delimiter=',')\n",
    "\n",
    "# Match the eval example ids with the AdaBoost predictions.\n",
    "eval_id_data.columns = ['id']\n",
    "adaboost_data = pd.concat([eval_id_data, adaboost_data], axis=1)\n",
    "\n",
    "# Get rid of the label column since we just want the example ids and the predictions.\n",
    "adaboost_data.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Rename the columns for submission.\n",
    "adaboost_data.columns = ['example_id', 'label']\n",
    "\n",
    "# Save the AdaBoost submission data out.\n",
    "adaboost_data.to_csv('output/adaboost_submission.csv', index=False)\n",
    "\n",
    "adaboost_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the next 3 cells back to back before running the SVM/Logistic Regression code.\n",
    "\n",
    "# Pre-process the train data for SVM/Logistic Regression.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use this scaler to normalize the train data.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "p_x = data.iloc[:, 1:]\n",
    "p_y = data.iloc[:, 0]\n",
    "p_x_norm = scaler.fit_transform(p_x)\n",
    "\n",
    "pd.concat([p_y, pd.DataFrame(p_x_norm)], axis=1).to_csv('output/svm_logreg_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the test data for SVM/Logistic Regression.\n",
    "test_data = pd.read_csv('project_data/data/test.csv', delimiter=',')\n",
    "\n",
    "p_test_x = test_data.iloc[:, 1:]\n",
    "p_test_y = test_data.iloc[:, 0]\n",
    "p_test_x_norm = scaler.transform(p_test_x)\n",
    "\n",
    "pd.concat([p_test_y, pd.DataFrame(p_test_x_norm)], axis=1).to_csv('output/svm_logreg_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the eval data for SVM/Logistic Regression.\n",
    "eval_data = pd.read_csv('project_data/data/eval.anon.csv', delimiter=',')\n",
    "\n",
    "p_eval_x = eval_data.iloc[:, 1:]\n",
    "p_eval_y = eval_data.iloc[:, 0]\n",
    "p_eval_x_norm = scaler.transform(p_eval_x)\n",
    "\n",
    "pd.concat([p_eval_y, pd.DataFrame(p_eval_x_norm)], axis=1).to_csv('output/svm_logreg_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the SVM code in the svm_logreg directory to get the predictions first.\n",
    "\n",
    "# Post-process the SVM predictions for submission.\n",
    "eval_id_data = pd.read_csv('project_data/data/eval.id', header=None)\n",
    "svm_data = pd.read_csv('svm_logreg/svm_logreg_preds_no_id.csv', delimiter=',')\n",
    "\n",
    "# Match the eval example ids with the SVM predictions.\n",
    "eval_id_data.columns = ['id']\n",
    "svm_data = pd.concat([eval_id_data, svm_data], axis=1)\n",
    "\n",
    "# Get rid of the label column since we just want the example ids and the predictions.\n",
    "svm_data.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Rename the columns for submission.\n",
    "svm_data.columns = ['example_id', 'label']\n",
    "\n",
    "# Save the SVM submission data out.\n",
    "svm_data.to_csv('output/svm_submission.csv', index=False)\n",
    "\n",
    "svm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the Logistic Regression code in the svm_logreg directory to get the predictions first.\n",
    "\n",
    "# Post-process the Logistic Regression predictions for submission.\n",
    "\n",
    "eval_id_data = pd.read_csv('project_data/data/eval.id', header=None)\n",
    "logreg_data = pd.read_csv('svm_logreg/svm_logreg_preds_no_id.csv', delimiter=',')\n",
    "\n",
    "# Match the eval example ids with the Logistic Regression predictions.\n",
    "eval_id_data.columns = ['id']\n",
    "logreg_data = pd.concat([eval_id_data, logreg_data], axis=1)\n",
    "\n",
    "# Get rid of the label column since we just want the example ids and the predictions.\n",
    "logreg_data.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Rename the columns for submission.\n",
    "logreg_data.columns = ['example_id', 'label']\n",
    "\n",
    "# Save the Logistic Regression submission data out.\n",
    "logreg_data.to_csv('output/logreg_submission.csv', index=False)\n",
    "\n",
    "logreg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the next 3 cells back to back before running the perceptron code (non-trivial pre-processing).\n",
    "\n",
    "# Pre-process the train data for Perceptron (non-trivial pre-processing).\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Use this scaler to normalize the train data.\n",
    "scaler = StandardScaler()\n",
    "vt = VarianceThreshold(0.01)\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "p_x = data.iloc[:, 1:]\n",
    "p_y = data.iloc[:, 0]\n",
    "p_x = vt.fit_transform(p_x)\n",
    "p_x = scaler.fit_transform(p_x)\n",
    "\n",
    "p_x_resampled, p_y_resampled = smote.fit_resample(p_x, p_y)\n",
    "\n",
    "# Replace 0 with -1 to fit design of custom perceptron implementation.\n",
    "p_y_resampled = p_y_resampled.replace(0, -1)\n",
    "\n",
    "pd.concat([p_y_resampled, pd.DataFrame(p_x_resampled)], axis=1).to_csv('output/perceptron_train_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the test data for Perceptron (non-trivial pre-processing).\n",
    "test_data = pd.read_csv('project_data/data/test.csv', delimiter=',')\n",
    "\n",
    "p_test_x = test_data.iloc[:, 1:]\n",
    "p_test_y = test_data.iloc[:, 0]\n",
    "p_test_x = vt.transform(p_test_x)\n",
    "p_test_x = scaler.transform(p_test_x)\n",
    "\n",
    "# Replace 0 with -1 to fit design of custom perceptron implementation.\n",
    "p_test_y = p_test_y.replace(0, -1)\n",
    "\n",
    "pd.concat([p_test_y, pd.DataFrame(p_test_x)], axis=1).to_csv('output/perceptron_test_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the eval data for Perceptron (non-trivial pre-processing).\n",
    "eval_data = pd.read_csv('project_data/data/eval.anon.csv', delimiter=',')\n",
    "\n",
    "p_eval_x = eval_data.iloc[:, 1:]\n",
    "p_eval_y = eval_data.iloc[:, 0]\n",
    "p_eval_x = vt.transform(p_eval_x)\n",
    "p_eval_x = scaler.transform(p_eval_x)\n",
    "\n",
    "# Replace 0 with -1 to fit design of custom perceptron implementation.\n",
    "p_eval_y = p_eval_y.replace(0, -1)\n",
    "\n",
    "pd.concat([p_eval_y, pd.DataFrame(p_eval_x)], axis=1).to_csv('output/perceptron_eval_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run the perceptron code in the perceptron directory to get the predictions first.\n",
    "# NOTE: Change the train and test data path in perceptron/data.py to the smote data.\n",
    "\n",
    "# Post-process the perceptron predictions for submission.\n",
    "eval_id_data = pd.read_csv('project_data/data/eval.id', header=None)\n",
    "perceptron_margin_smote_data = pd.read_csv('perceptron/perceptron_preds_no_id.csv', delimiter=',')\n",
    "\n",
    "# Match the eval example ids with the perceptron predictions.\n",
    "eval_id_data.columns = ['id']\n",
    "perceptron_margin_smote_data = pd.concat([eval_id_data, perceptron_margin_smote_data], axis=1)\n",
    "\n",
    "# Get rid of the label column since we just want the example ids and the predictions.\n",
    "perceptron_margin_smote_data.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Rename the columns for submission.\n",
    "perceptron_margin_smote_data.columns = ['example_id', 'label']\n",
    "\n",
    "# Save the perceptron submission data out.\n",
    "perceptron_margin_smote_data.to_csv('output/perceptron_margin_smote_submission.csv', index=False)\n",
    "\n",
    "perceptron_margin_smote_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6640",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
